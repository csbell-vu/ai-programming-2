{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2a28162-8f3e-42f8-83f6-1b9ef38cd394"
      },
      "source": [
        "# AI-Assisted Programming with Langchain\n",
        "> Creating full-featured apps\n",
        "\n",
        "We've already talked about general programming, transformers, and openai. Let's see where this fits in."
      ],
      "id": "a2a28162-8f3e-42f8-83f6-1b9ef38cd394"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c136a85e-35df-48ac-849a-79960975a6b9"
      },
      "source": [
        "# AI Ecosystem"
      ],
      "id": "c136a85e-35df-48ac-849a-79960975a6b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88934c0a-c340-4678-be23-563a4fe331f1"
      },
      "source": [
        "# Langchain\n",
        "\n",
        "[Overview of Langchain](https://python.langchain.com/docs/get_started/introduction)"
      ],
      "id": "88934c0a-c340-4678-be23-563a4fe331f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1cebcf-1ac1-48aa-b998-9f0b2a77567f"
      },
      "source": [
        "## Creating and Prompting an LLM"
      ],
      "id": "8c1cebcf-1ac1-48aa-b998-9f0b2a77567f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15202e4f-5a92-4231-a557-43daa913beb5"
      },
      "outputs": [],
      "source": [
        "!pip install gradio langchain\n",
        "!pip install pypdf chromadb transformers\n",
        "!pip install duckduckgo-search"
      ],
      "id": "15202e4f-5a92-4231-a557-43daa913beb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c48401e1-8b8c-4a7e-9a70-102991845730"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "\n",
        "from free_access_models import ChatMistralLocal, LLMMistralLocal, MistralEmbeddingsLocal\n",
        "import numpy as np"
      ],
      "id": "c48401e1-8b8c-4a7e-9a70-102991845730"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF31zRWdqRlT"
      },
      "outputs": [],
      "source": [
        "# auth replicated here for reference just in case you choose to do something similar\n",
        "#from google.colab import userdata\n",
        "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "#os.environ['HF_TOKEN'] = os.environ['HUGGINGFACEHUB_API_TOKEN']"
      ],
      "id": "tF31zRWdqRlT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D0GyPPpedXw"
      },
      "source": [
        "# Basic Prompt Chains\n",
        "\n",
        "See [Prompt+LLM](https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser) for more information"
      ],
      "id": "2D0GyPPpedXw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f068a62-9a56-420a-be6f-2bf3bb805d04"
      },
      "outputs": [],
      "source": [
        "# basic usage\n"
      ],
      "id": "4f068a62-9a56-420a-be6f-2bf3bb805d04"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GVTTau0fsdd"
      },
      "outputs": [],
      "source": [
        "# Observe what the prompt looks like when we substitute words into it\n"
      ],
      "id": "1GVTTau0fsdd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP7khR3zd2OJ"
      },
      "outputs": [],
      "source": [
        "# Now, actually call the entire chain on it\n"
      ],
      "id": "VP7khR3zd2OJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUCuzx2HfeVx"
      },
      "source": [
        "## Even more simplified prompt chains"
      ],
      "id": "LUCuzx2HfeVx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM3wDIpnfg3f"
      },
      "outputs": [],
      "source": [
        "# Create total user prompt chain\n",
        "prompt = ChatPromptTemplate.from_template(\"\")\n",
        "chain = prompt | model"
      ],
      "id": "eM3wDIpnfg3f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hStGS1kfonX"
      },
      "outputs": [],
      "source": [
        "# Now, the user can submit literally whatever\n"
      ],
      "id": "8hStGS1kfonX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-T1I1nDiM-A"
      },
      "source": [
        "# Creating and Using a Knowledge Base : Retrieval Augmented Generation (RAG)\n",
        "\n",
        "* Conceptual and step-by-step guide about [RAG](https://python.langchain.com/docs/modules/data_connection/)\n",
        "* Learn more about implementing [RAG](https://python.langchain.com/docs/expression_language/cookbook/retrieval)"
      ],
      "id": "j-T1I1nDiM-A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUeLCri-kojW"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from free_access_models import MistralEmbeddingsLocal"
      ],
      "id": "aUeLCri-kojW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6J_j3oiiSVx"
      },
      "outputs": [],
      "source": [
        "knowledge_base = 'Pro-Se-Handbook-APPROVED-v2019-2.pdf'"
      ],
      "id": "X6J_j3oiiSVx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOs-D7XQlmzt"
      },
      "source": [
        "## Document Loaders and Splitters"
      ],
      "id": "HOs-D7XQlmzt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk4B1Ihklod8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n"
      ],
      "id": "Sk4B1Ihklod8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeLnP9b_nKRC"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of Documents: {len(pages)}\\n\")\n",
        "pages[:2]"
      ],
      "id": "WeLnP9b_nKRC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKdzgo6ci0MD"
      },
      "source": [
        "## Vector Stores: A way to store embeddings (hidden states) of your data"
      ],
      "id": "mKdzgo6ci0MD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmXDTgoqjCUs"
      },
      "outputs": [],
      "source": [],
      "id": "jmXDTgoqjCUs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrievers: How we select the most relevant data"
      ],
      "metadata": {
        "id": "lt_uoYfLjUdx"
      },
      "id": "lt_uoYfLjUdx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC-5zEeioFmq"
      },
      "outputs": [],
      "source": [
        "# query the vector store\n",
        "query = 'What forms do I need to fill out to begin with?'\n",
        "\n",
        "# use a similarity search between the vectors\n",
        "\n"
      ],
      "id": "XC-5zEeioFmq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWJwyByyhoSB"
      },
      "id": "CWJwyByyhoSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or use the db as a retriever\n"
      ],
      "metadata": {
        "id": "KTNxqQf7kiY_"
      },
      "id": "KTNxqQf7kiY_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG\n",
        "For when we want to actually do generation, but want there to be retrieved documents included"
      ],
      "metadata": {
        "id": "8IuaToUPc0Hy"
      },
      "id": "8IuaToUPc0Hy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic question answering template\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# compose prompt\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# retriever already created above"
      ],
      "metadata": {
        "id": "Fsewtgf9alyl"
      },
      "id": "Fsewtgf9alyl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ExSC1rNNmCw"
      },
      "outputs": [],
      "source": [
        "# create the chain\n"
      ],
      "id": "8ExSC1rNNmCw"
    },
    {
      "cell_type": "code",
      "source": [
        "# get the response\n"
      ],
      "metadata": {
        "id": "KOVFJ-Bjlgt9"
      },
      "id": "KOVFJ-Bjlgt9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNHwPEDUex2e"
      },
      "source": [
        "# Agents\n",
        "Learn more about [Agents](https://python.langchain.com/docs/modules/agents/quick_start) in their Quickstart"
      ],
      "id": "iNHwPEDUex2e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose Tools"
      ],
      "metadata": {
        "id": "d98DLIeSsAI6"
      },
      "id": "d98DLIeSsAI6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzJW21MTO1xB"
      },
      "outputs": [],
      "source": [
        "# Web search tools\n",
        "from langchain.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "# Creating custom retriever tools\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# For tested prompts\n",
        "from langchain import hub\n",
        "\n",
        "# For agents\n",
        "from langchain.agents import create_react_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n"
      ],
      "id": "CzJW21MTO1xB"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Clt4Dk0osLND"
      },
      "id": "Clt4Dk0osLND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# based off of the Guide we used\n"
      ],
      "metadata": {
        "id": "q3WReEmJuHKu"
      },
      "id": "q3WReEmJuHKu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tools\n"
      ],
      "metadata": {
        "id": "oIYU5mI1sR4r"
      },
      "id": "oIYU5mI1sR4r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
        "    HumanMessagePromptTemplate.from_template(template='Answer the following questions as best you can or reply with the most meaningful ' +\n",
        "                                             'response possible. You have access to the following tools:\\n\\n{tools}\\n\\n although you can also reply conversationally when appropriate. '\n",
        "                                             'Use the following format:\\n\\n' +\n",
        "                                             'Question: the input question you must answer or statement to which you should reply\\n' +\n",
        "                                             'Thought: you should always think about what to do\\n' +\n",
        "                                             'Action: the action to take, should be one of [{tool_names}]\\n'+\n",
        "                                             'Action Input: the input to the action\\n'+\n",
        "                                             'Observation: the result of the action\\n... '+\n",
        "                                             '(this Thought/Action/Action Input/Observation can repeat up to 2 times)\\n'+\n",
        "                                             'Thought: I now know the final answer\\n'+\n",
        "                                             'Final Answer: the final answer to the original input question or appropriate response is\\n\\n'+\n",
        "                                             'Begin!\\n\\nQuestion or message: {input}\\nThought:{agent_scratchpad}'),\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "zvZqqRP455hQ"
      },
      "id": "zvZqqRP455hQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our own llm and agent\n"
      ],
      "metadata": {
        "id": "jJKVFnLTvBVZ"
      },
      "id": "jJKVFnLTvBVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the agent\n"
      ],
      "metadata": {
        "id": "pOb4kdkWvWlZ"
      },
      "id": "pOb4kdkWvWlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding the conversational component"
      ],
      "metadata": {
        "id": "ZUKUZASfxeSK"
      },
      "id": "ZUKUZASfxeSK"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "BuxC13qOx9pG"
      },
      "id": "BuxC13qOx9pG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start with empty chat history. We can pass this as a parameter in the invoke\n",
        "\n",
        "\n",
        "# Create the agent with chat history\n"
      ],
      "metadata": {
        "id": "QZce0riMv0p9"
      },
      "id": "QZce0riMv0p9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start chatting away\n",
        "agent_with_chat_history.invoke(\n",
        "    {\"input\": \"I'm going pro se on a civil case against a corporation that scammed me repeatedly. \"+\n",
        "     \"How should I start preparing? I'm in the state of Tennesse.\"},\n",
        "    config={\"configurable\": {\"session_id\": '<something>'}},\n",
        ")"
      ],
      "metadata": {
        "id": "C0zUKZQJyLfD"
      },
      "id": "C0zUKZQJyLfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_with_chat_history.invoke(\n",
        "    {\"input\": \"Thank you. What do you think will be the most difficult part of representing myself in court?\"},\n",
        "    # This is needed because in most real world scenarios, a session id is needed\n",
        "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
        "    config={\"configurable\": {\"session_id\": \"<something>\"}},\n",
        ")"
      ],
      "metadata": {
        "id": "nP1aUsIWykx3"
      },
      "id": "nP1aUsIWykx3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bringing it all together"
      ],
      "metadata": {
        "id": "Pxd4XCJhzS7U"
      },
      "id": "Pxd4XCJhzS7U"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "C5k1KMbNzSj4"
      },
      "id": "C5k1KMbNzSj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_response(message, history):\n",
        "    agent_message = {'input':message}\n",
        "    response = agent_with_chat_history.invoke(agent_message, config={\"configurable\": {\"session_id\": history}})\n",
        "    return response['output']\n",
        "\n",
        "with gr.Blocks() as demo :\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    msg.submit(ai_response, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "W8HOzqAA1VJg"
      },
      "id": "W8HOzqAA1VJg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}